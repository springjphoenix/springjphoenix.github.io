{"version":3,"sources":["webpack:///path---iamtrask-anyone-can-code-lstm-b0eda0efd4b3ed20fafa.js","webpack:///./.cache/json/iamtrask-anyone-can-code-lstm.json"],"names":["webpackJsonp","615","module","exports","data","site","siteMetadata","title","author","markdownRemark","id","html","timeToRead","frontmatter","date","category","tags","math","pathContext","prev","url","slug","next"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,MAAQC,cAAgBC,MAAA,iBAAAC,OAAA,YAA6CC,gBAAmBC,GAAA,wIAAAC,KAAA;AAAsm+BC,WAAA,EAAAC,aAAm/JN,MAAA,6BAAAO,KAAA,aAAAC,SAAA,KAAAC,MAAA,kCAAAC,KAAA,QAAiIC,aAAgBC,MAAQC,IAAA,sBAAAb,MAAA,qBAAwDc,KAAA,kCAAAC,MAAkDF,IAAA,0BAAAb,MAAA","file":"path---iamtrask-anyone-can-code-lstm-b0eda0efd4b3ed20fafa.js","sourcesContent":["webpackJsonp([190882071525408],{\n\n/***/ 615:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"Magicly's Blog\",\"author\":\"Magicly\"}},\"markdownRemark\":{\"id\":\"/Users/spring/Developer/Gatsby/springjphoenix.github.io/src/pages/iamtrask-anyone-can-code-lstm.md absPath of file >>> MarkdownRemark\",\"html\":\"<p>本文翻译自<a href=\\\"https://twitter.com/iamtrask\\\">@iamtrask</a>的<a href=\\\"http://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/\\\">Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN)</a>。本文作者已通过<a href=\\\"https://twitter.com/magicly007/with_replies\\\">twitter联系作者，获得授权</a>。</p>\\n<h1 id=\\\"概要\\\"><a href=\\\"#%E6%A6%82%E8%A6%81\\\" aria-hidden=\\\"true\\\" class=\\\"anchor\\\"><svg aria-hidden=\\\"true\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg></a>概要</h1>\\n<p>我通过玩具代码一边学习一边调试能达到最好的学习效果。本文通过一个简单的python实现，教会你循环神经网络。</p>\\n<p>原文作者<a href=\\\"https://twitter.com/iamtrask\\\">@iamtrask</a>说他会在twitter上继续发布第二部分LSTM，敬请关注。</p>\\n<!-- more -->\\n<h1 id=\\\"废话少说，-给我看看代码\\\"><a href=\\\"#%E5%BA%9F%E8%AF%9D%E5%B0%91%E8%AF%B4%EF%BC%8C-%E7%BB%99%E6%88%91%E7%9C%8B%E7%9C%8B%E4%BB%A3%E7%A0%81\\\" aria-hidden=\\\"true\\\" class=\\\"anchor\\\"><svg aria-hidden=\\\"true\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg></a>废话少说， 给我看看代码</h1>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">import</span> copy<span class=\\\"token punctuation\\\">,</span> numpy <span class=\\\"token keyword\\\">as</span> np\\nnp<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">.</span>seed<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#固定随机数生成器的种子，便于得到固定的输出，【译者注：完全是为了方便调试用的]</span>\\n\\n<span class=\\\"token comment\\\"># compute sigmoid nonlinearity</span>\\n<span class=\\\"token keyword\\\">def</span> <span class=\\\"token function\\\">sigmoid</span><span class=\\\"token punctuation\\\">(</span>x<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span> <span class=\\\"token comment\\\">#激活函数</span>\\n    output <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">1</span><span class=\\\"token operator\\\">/</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">1</span><span class=\\\"token operator\\\">+</span>np<span class=\\\"token punctuation\\\">.</span>exp<span class=\\\"token punctuation\\\">(</span><span class=\\\"token operator\\\">-</span>x<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span>\\n    <span class=\\\"token keyword\\\">return</span> output\\n\\n<span class=\\\"token comment\\\"># convert output of sigmoid function to its derivative</span>\\n<span class=\\\"token keyword\\\">def</span> <span class=\\\"token function\\\">sigmoid_output_to_derivative</span><span class=\\\"token punctuation\\\">(</span>output<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span><span class=\\\"token comment\\\">#激活函数的导数</span>\\n    <span class=\\\"token keyword\\\">return</span> output<span class=\\\"token operator\\\">*</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">1</span><span class=\\\"token operator\\\">-</span>output<span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># training dataset generation</span>\\nint2binary <span class=\\\"token operator\\\">=</span> <span class=\\\"token punctuation\\\">{</span><span class=\\\"token punctuation\\\">}</span> <span class=\\\"token comment\\\">#整数到其二进制表示的映射</span>\\nbinary_dim <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">8</span> <span class=\\\"token comment\\\">#暂时制作256以内的加法， 可以调大</span>\\n\\n<span class=\\\"token comment\\\">## 以下5行代码计算0-256的二进制表示</span>\\nlargest_number <span class=\\\"token operator\\\">=</span> <span class=\\\"token builtin\\\">pow</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">2</span><span class=\\\"token punctuation\\\">,</span>binary_dim<span class=\\\"token punctuation\\\">)</span>\\nbinary <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>unpackbits<span class=\\\"token punctuation\\\">(</span>\\n    np<span class=\\\"token punctuation\\\">.</span>array<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token builtin\\\">range</span><span class=\\\"token punctuation\\\">(</span>largest_number<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">,</span>dtype<span class=\\\"token operator\\\">=</span>np<span class=\\\"token punctuation\\\">.</span>uint8<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">.</span>T<span class=\\\"token punctuation\\\">,</span>axis<span class=\\\"token operator\\\">=</span><span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">)</span>\\n<span class=\\\"token keyword\\\">for</span> i <span class=\\\"token keyword\\\">in</span> <span class=\\\"token builtin\\\">range</span><span class=\\\"token punctuation\\\">(</span>largest_number<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span>\\n    int2binary<span class=\\\"token punctuation\\\">[</span>i<span class=\\\"token punctuation\\\">]</span> <span class=\\\"token operator\\\">=</span> binary<span class=\\\"token punctuation\\\">[</span>i<span class=\\\"token punctuation\\\">]</span>\\n\\n<span class=\\\"token comment\\\"># input variables</span>\\nalpha <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">0.1</span> <span class=\\\"token comment\\\">#学习速率</span>\\ninput_dim <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">2</span> <span class=\\\"token comment\\\">#因为我们是做两个数相加，每次会喂给神经网络两个bit，所以输入的维度是2</span>\\nhidden_dim <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">16</span> <span class=\\\"token comment\\\">#隐藏层的神经元节点数，远比理论值要大（译者注：理论上而言，应该一个节点就可以记住有无进位了，但我试了发现4的时候都没法收敛），你可以自己调整这个数，看看调大了是容易更快地收敛还是更慢</span>\\noutput_dim <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">1</span> <span class=\\\"token comment\\\">#我们的输出是一个数，所以维度为1</span>\\n\\n\\n<span class=\\\"token comment\\\"># initialize neural network weights</span>\\nsynapse_0 <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">2</span><span class=\\\"token operator\\\">*</span>np<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">(</span>input_dim<span class=\\\"token punctuation\\\">,</span>hidden_dim<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">-</span> <span class=\\\"token number\\\">1</span> <span class=\\\"token comment\\\">#输入层到隐藏层的转化矩阵，维度为2*16， 2是输入维度，16是隐藏层维度</span>\\nsynapse_1 <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">2</span><span class=\\\"token operator\\\">*</span>np<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">(</span>hidden_dim<span class=\\\"token punctuation\\\">,</span>output_dim<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">-</span> <span class=\\\"token number\\\">1</span>\\nsynapse_h <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">2</span><span class=\\\"token operator\\\">*</span>np<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">(</span>hidden_dim<span class=\\\"token punctuation\\\">,</span>hidden_dim<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">-</span> <span class=\\\"token number\\\">1</span>\\n<span class=\\\"token comment\\\"># 译者注：np.random.random产生的是[0,1)的随机数，2 * [0, 1) - 1 => [-1, 1)，</span>\\n<span class=\\\"token comment\\\"># 是为了有正有负更快地收敛，这涉及到如何初始化参数的问题，通常来说都是靠“经验”或者说“启发式规则”，说得直白一点就是“蒙的”！机器学习里面，超参数的选择，大部分都是这种情况，哈哈。。。</span>\\n<span class=\\\"token comment\\\"># 我自己试了一下用【0, 2)之间的随机数，貌似不能收敛，用[0,1)就可以，呵呵。。。</span>\\n\\n<span class=\\\"token comment\\\"># 以下三个分别对应三个矩阵的变化</span>\\nsynapse_0_update <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>zeros_like<span class=\\\"token punctuation\\\">(</span>synapse_0<span class=\\\"token punctuation\\\">)</span>\\nsynapse_1_update <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>zeros_like<span class=\\\"token punctuation\\\">(</span>synapse_1<span class=\\\"token punctuation\\\">)</span>\\nsynapse_h_update <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>zeros_like<span class=\\\"token punctuation\\\">(</span>synapse_h<span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># training logic</span>\\n<span class=\\\"token comment\\\"># 学习10000个例子</span>\\n<span class=\\\"token keyword\\\">for</span> j <span class=\\\"token keyword\\\">in</span> <span class=\\\"token builtin\\\">range</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">100000</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span>\\n    \\n    <span class=\\\"token comment\\\"># 下面6行代码，随机产生两个0-128的数字，并查出他们的二进制表示。为了避免相加之和超过256，这里选择两个0-128的数字</span>\\n    <span class=\\\"token comment\\\"># generate a simple addition problem (a + b = c)</span>\\n    a_int <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">.</span>randint<span class=\\\"token punctuation\\\">(</span>largest_number<span class=\\\"token operator\\\">/</span><span class=\\\"token number\\\">2</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\"># int version</span>\\n    a <span class=\\\"token operator\\\">=</span> int2binary<span class=\\\"token punctuation\\\">[</span>a_int<span class=\\\"token punctuation\\\">]</span> <span class=\\\"token comment\\\"># binary encoding</span>\\n\\n    b_int <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">.</span>randint<span class=\\\"token punctuation\\\">(</span>largest_number<span class=\\\"token operator\\\">/</span><span class=\\\"token number\\\">2</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\"># int version</span>\\n    b <span class=\\\"token operator\\\">=</span> int2binary<span class=\\\"token punctuation\\\">[</span>b_int<span class=\\\"token punctuation\\\">]</span> <span class=\\\"token comment\\\"># binary encoding</span>\\n\\n    <span class=\\\"token comment\\\"># true answer</span>\\n    c_int <span class=\\\"token operator\\\">=</span> a_int <span class=\\\"token operator\\\">+</span> b_int\\n    c <span class=\\\"token operator\\\">=</span> int2binary<span class=\\\"token punctuation\\\">[</span>c_int<span class=\\\"token punctuation\\\">]</span>\\n    \\n    <span class=\\\"token comment\\\"># where we'll store our best guess (binary encoded)</span>\\n    <span class=\\\"token comment\\\"># 存储神经网络的预测值</span>\\n    d <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>zeros_like<span class=\\\"token punctuation\\\">(</span>c<span class=\\\"token punctuation\\\">)</span>\\n\\n    overallError <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">0</span> <span class=\\\"token comment\\\">#每次把总误差清零</span>\\n    \\n    layer_2_deltas <span class=\\\"token operator\\\">=</span> <span class=\\\"token builtin\\\">list</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#存储每个时间点输出层的误差</span>\\n    layer_1_values <span class=\\\"token operator\\\">=</span> <span class=\\\"token builtin\\\">list</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#存储每个时间点隐藏层的值</span>\\n    layer_1_values<span class=\\\"token punctuation\\\">.</span>append<span class=\\\"token punctuation\\\">(</span>np<span class=\\\"token punctuation\\\">.</span>zeros<span class=\\\"token punctuation\\\">(</span>hidden_dim<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#一开始没有隐藏层，所以里面都是0</span>\\n    \\n    <span class=\\\"token comment\\\"># moving along the positions in the binary encoding</span>\\n    <span class=\\\"token keyword\\\">for</span> position <span class=\\\"token keyword\\\">in</span> <span class=\\\"token builtin\\\">range</span><span class=\\\"token punctuation\\\">(</span>binary_dim<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span><span class=\\\"token comment\\\">#循环遍历每一个二进制位</span>\\n        \\n        <span class=\\\"token comment\\\"># generate input and output</span>\\n        X <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>array<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token punctuation\\\">[</span>a<span class=\\\"token punctuation\\\">[</span>binary_dim <span class=\\\"token operator\\\">-</span> position <span class=\\\"token operator\\\">-</span> <span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">,</span>b<span class=\\\"token punctuation\\\">[</span>binary_dim <span class=\\\"token operator\\\">-</span> position <span class=\\\"token operator\\\">-</span> <span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token comment\\\">#从右到左，每次去两个输入数字的一个bit位</span>\\n        y <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>array<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token punctuation\\\">[</span>c<span class=\\\"token punctuation\\\">[</span>binary_dim <span class=\\\"token operator\\\">-</span> position <span class=\\\"token operator\\\">-</span> <span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">.</span>T<span class=\\\"token comment\\\">#正确答案</span>\\n\\n        <span class=\\\"token comment\\\"># hidden layer (input ~+ prev_hidden)</span>\\n        layer_1 <span class=\\\"token operator\\\">=</span> sigmoid<span class=\\\"token punctuation\\\">(</span>np<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>X<span class=\\\"token punctuation\\\">,</span>synapse_0<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">+</span> np<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>layer_1_values<span class=\\\"token punctuation\\\">[</span><span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">,</span>synapse_h<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token comment\\\">#（输入层 + 之前的隐藏层） -> 新的隐藏层，这是体现循环神经网络的最核心的地方！！！</span>\\n\\n        <span class=\\\"token comment\\\"># output layer (new binary representation)</span>\\n        layer_2 <span class=\\\"token operator\\\">=</span> sigmoid<span class=\\\"token punctuation\\\">(</span>np<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>layer_1<span class=\\\"token punctuation\\\">,</span>synapse_1<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#隐藏层 * 隐藏层到输出层的转化矩阵synapse_1 -> 输出层</span>\\n\\n        <span class=\\\"token comment\\\"># did we miss?... if so, by how much?</span>\\n        layer_2_error <span class=\\\"token operator\\\">=</span> y <span class=\\\"token operator\\\">-</span> layer_2 <span class=\\\"token comment\\\">#预测误差是多少</span>\\n        layer_2_deltas<span class=\\\"token punctuation\\\">.</span>append<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">(</span>layer_2_error<span class=\\\"token punctuation\\\">)</span><span class=\\\"token operator\\\">*</span>sigmoid_output_to_derivative<span class=\\\"token punctuation\\\">(</span>layer_2<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#我们把每一个时间点的误差导数都记录下来</span>\\n        overallError <span class=\\\"token operator\\\">+=</span> np<span class=\\\"token punctuation\\\">.</span><span class=\\\"token builtin\\\">abs</span><span class=\\\"token punctuation\\\">(</span>layer_2_error<span class=\\\"token punctuation\\\">[</span><span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token comment\\\">#总误差</span>\\n    \\n        <span class=\\\"token comment\\\"># decode estimate so we can print it out</span>\\n        d<span class=\\\"token punctuation\\\">[</span>binary_dim <span class=\\\"token operator\\\">-</span> position <span class=\\\"token operator\\\">-</span> <span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span> <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span><span class=\\\"token builtin\\\">round</span><span class=\\\"token punctuation\\\">(</span>layer_2<span class=\\\"token punctuation\\\">[</span><span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#记录下每一个预测bit位</span>\\n        \\n        <span class=\\\"token comment\\\"># store hidden layer so we can use it in the next timestep</span>\\n        layer_1_values<span class=\\\"token punctuation\\\">.</span>append<span class=\\\"token punctuation\\\">(</span>copy<span class=\\\"token punctuation\\\">.</span>deepcopy<span class=\\\"token punctuation\\\">(</span>layer_1<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token comment\\\">#记录下隐藏层的值，在下一个时间点用</span>\\n    \\n    future_layer_1_delta <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>zeros<span class=\\\"token punctuation\\\">(</span>hidden_dim<span class=\\\"token punctuation\\\">)</span>\\n    \\n    <span class=\\\"token comment\\\">#前面代码我们完成了所有时间点的正向传播以及计算最后一层的误差，现在我们要做的是反向传播，从最后一个时间点到第一个时间点</span>\\n    <span class=\\\"token keyword\\\">for</span> position <span class=\\\"token keyword\\\">in</span> <span class=\\\"token builtin\\\">range</span><span class=\\\"token punctuation\\\">(</span>binary_dim<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span>\\n        \\n        X <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>array<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token punctuation\\\">[</span>a<span class=\\\"token punctuation\\\">[</span>position<span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">,</span>b<span class=\\\"token punctuation\\\">[</span>position<span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#最后一次的两个输入</span>\\n        layer_1 <span class=\\\"token operator\\\">=</span> layer_1_values<span class=\\\"token punctuation\\\">[</span><span class=\\\"token operator\\\">-</span>position<span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span> <span class=\\\"token comment\\\">#当前时间点的隐藏层</span>\\n        prev_layer_1 <span class=\\\"token operator\\\">=</span> layer_1_values<span class=\\\"token punctuation\\\">[</span><span class=\\\"token operator\\\">-</span>position<span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">2</span><span class=\\\"token punctuation\\\">]</span> <span class=\\\"token comment\\\">#前一个时间点的隐藏层</span>\\n        \\n        <span class=\\\"token comment\\\"># error at output layer</span>\\n        layer_2_delta <span class=\\\"token operator\\\">=</span> layer_2_deltas<span class=\\\"token punctuation\\\">[</span><span class=\\\"token operator\\\">-</span>position<span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span> <span class=\\\"token comment\\\">#当前时间点输出层导数</span>\\n        <span class=\\\"token comment\\\"># error at hidden layer</span>\\n        <span class=\\\"token comment\\\"># 通过后一个时间点（因为是反向传播）的隐藏层误差和当前时间点的输出层误差，计算当前时间点的隐藏层误差</span>\\n        layer_1_delta <span class=\\\"token operator\\\">=</span> <span class=\\\"token punctuation\\\">(</span>future_layer_1_delta<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>synapse_h<span class=\\\"token punctuation\\\">.</span>T<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">+</span> layer_2_delta<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>synapse_1<span class=\\\"token punctuation\\\">.</span>T<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> sigmoid_output_to_derivative<span class=\\\"token punctuation\\\">(</span>layer_1<span class=\\\"token punctuation\\\">)</span>\\n\\n        <span class=\\\"token comment\\\"># let's update all our weights so we can try again</span>\\n        <span class=\\\"token comment\\\"># 我们已经完成了当前时间点的反向传播误差计算， 可以构建更新矩阵了。但是我们并不会现在就更新权重矩阵，因为我们还要用他们计算前一个时间点的更新矩阵呢。</span>\\n        <span class=\\\"token comment\\\"># 所以要等到我们完成了所有反向传播误差计算， 才会真正的去更新权重矩阵，我们暂时把更新矩阵存起来。</span>\\n        <span class=\\\"token comment\\\"># 可以看这里了解更多关于反向传播的知识http://iamtrask.github.io/2015/07/12/basic-python-network/</span>\\n        synapse_1_update <span class=\\\"token operator\\\">+=</span> np<span class=\\\"token punctuation\\\">.</span>atleast_2d<span class=\\\"token punctuation\\\">(</span>layer_1<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">.</span>T<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>layer_2_delta<span class=\\\"token punctuation\\\">)</span>\\n        synapse_h_update <span class=\\\"token operator\\\">+=</span> np<span class=\\\"token punctuation\\\">.</span>atleast_2d<span class=\\\"token punctuation\\\">(</span>prev_layer_1<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">.</span>T<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>layer_1_delta<span class=\\\"token punctuation\\\">)</span>\\n        synapse_0_update <span class=\\\"token operator\\\">+=</span> X<span class=\\\"token punctuation\\\">.</span>T<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>layer_1_delta<span class=\\\"token punctuation\\\">)</span>\\n        \\n        future_layer_1_delta <span class=\\\"token operator\\\">=</span> layer_1_delta\\n    \\n\\n    <span class=\\\"token comment\\\"># 我们已经完成了所有的反向传播，可以更新几个转换矩阵了。并把更新矩阵变量清零</span>\\n    synapse_0 <span class=\\\"token operator\\\">+=</span> synapse_0_update <span class=\\\"token operator\\\">*</span> alpha\\n    synapse_1 <span class=\\\"token operator\\\">+=</span> synapse_1_update <span class=\\\"token operator\\\">*</span> alpha\\n    synapse_h <span class=\\\"token operator\\\">+=</span> synapse_h_update <span class=\\\"token operator\\\">*</span> alpha\\n\\n    synapse_0_update <span class=\\\"token operator\\\">*=</span> <span class=\\\"token number\\\">0</span>\\n    synapse_1_update <span class=\\\"token operator\\\">*=</span> <span class=\\\"token number\\\">0</span>\\n    synapse_h_update <span class=\\\"token operator\\\">*=</span> <span class=\\\"token number\\\">0</span>\\n    \\n    <span class=\\\"token comment\\\"># print out progress</span>\\n    <span class=\\\"token keyword\\\">if</span><span class=\\\"token punctuation\\\">(</span>j <span class=\\\"token operator\\\">%</span> <span class=\\\"token number\\\">1000</span> <span class=\\\"token operator\\\">==</span> <span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span>\\n        <span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">\\\"Error:\\\"</span> <span class=\\\"token operator\\\">+</span> <span class=\\\"token builtin\\\">str</span><span class=\\\"token punctuation\\\">(</span>overallError<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span>\\n        <span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">\\\"Pred:\\\"</span> <span class=\\\"token operator\\\">+</span> <span class=\\\"token builtin\\\">str</span><span class=\\\"token punctuation\\\">(</span>d<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span>\\n        <span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">\\\"True:\\\"</span> <span class=\\\"token operator\\\">+</span> <span class=\\\"token builtin\\\">str</span><span class=\\\"token punctuation\\\">(</span>c<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span>\\n        out <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">0</span>\\n        <span class=\\\"token keyword\\\">for</span> index<span class=\\\"token punctuation\\\">,</span>x <span class=\\\"token keyword\\\">in</span> <span class=\\\"token builtin\\\">enumerate</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token builtin\\\">reversed</span><span class=\\\"token punctuation\\\">(</span>d<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span>\\n            out <span class=\\\"token operator\\\">+=</span> x<span class=\\\"token operator\\\">*</span><span class=\\\"token builtin\\\">pow</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">2</span><span class=\\\"token punctuation\\\">,</span>index<span class=\\\"token punctuation\\\">)</span>\\n        <span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token builtin\\\">str</span><span class=\\\"token punctuation\\\">(</span>a_int<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">+</span> <span class=\\\"token string\\\">\\\" + \\\"</span> <span class=\\\"token operator\\\">+</span> <span class=\\\"token builtin\\\">str</span><span class=\\\"token punctuation\\\">(</span>b_int<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">+</span> <span class=\\\"token string\\\">\\\" = \\\"</span> <span class=\\\"token operator\\\">+</span> <span class=\\\"token builtin\\\">str</span><span class=\\\"token punctuation\\\">(</span>out<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span>\\n        <span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">\\\"------------\\\"</span><span class=\\\"token punctuation\\\">)</span></code></pre>\\n      </div>\\n<h1 id=\\\"运行时输出\\\"><a href=\\\"#%E8%BF%90%E8%A1%8C%E6%97%B6%E8%BE%93%E5%87%BA\\\" aria-hidden=\\\"true\\\" class=\\\"anchor\\\"><svg aria-hidden=\\\"true\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg></a>运行时输出</h1>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">Error:[ 3.45638663]\\nPred:[0 0 0 0 0 0 0 1]\\nTrue:[0 1 0 0 0 1 0 1]\\n9 + 60 = 1\\n------------\\nError:[ 3.63389116]\\nPred:[1 1 1 1 1 1 1 1]\\nTrue:[0 0 1 1 1 1 1 1]\\n28 + 35 = 255\\n------------\\nError:[ 3.91366595]\\nPred:[0 1 0 0 1 0 0 0]\\nTrue:[1 0 1 0 0 0 0 0]\\n116 + 44 = 72\\n------------\\nError:[ 3.72191702]\\nPred:[1 1 0 1 1 1 1 1]\\nTrue:[0 1 0 0 1 1 0 1]\\n4 + 73 = 223\\n------------\\nError:[ 3.5852713]\\nPred:[0 0 0 0 1 0 0 0]\\nTrue:[0 1 0 1 0 0 1 0]\\n71 + 11 = 8\\n------------\\nError:[ 2.53352328]\\nPred:[1 0 1 0 0 0 1 0]\\nTrue:[1 1 0 0 0 0 1 0]\\n81 + 113 = 162\\n------------\\nError:[ 0.57691441]\\nPred:[0 1 0 1 0 0 0 1]\\nTrue:[0 1 0 1 0 0 0 1]\\n81 + 0 = 81\\n------------\\nError:[ 1.42589952]\\nPred:[1 0 0 0 0 0 0 1]\\nTrue:[1 0 0 0 0 0 0 1]\\n4 + 125 = 129\\n------------\\nError:[ 0.47477457]\\nPred:[0 0 1 1 1 0 0 0]\\nTrue:[0 0 1 1 1 0 0 0]\\n39 + 17 = 56\\n------------\\nError:[ 0.21595037]\\nPred:[0 0 0 0 1 1 1 0]\\nTrue:[0 0 0 0 1 1 1 0]\\n11 + 3 = 14\\n------------</code></pre>\\n      </div>\\n<h1 id=\\\"第一部分：什么是神经元记忆\\\"><a href=\\\"#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E5%85%83%E8%AE%B0%E5%BF%86\\\" aria-hidden=\\\"true\\\" class=\\\"anchor\\\"><svg aria-hidden=\\\"true\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg></a>第一部分：什么是神经元记忆</h1>\\n<p>顺着背出字母表，你很容易做到吧？</p>\\n<p>倒着背呢， 有点难哦。</p>\\n<p>试着想一首你记得的歌词。为什么顺着回忆比倒着回忆难？你能直接跳到第二小节的中间么？额， 好像有点难。 这是为什么呢？</p>\\n<p>这其实很符合逻辑。 你记忆字母表或者歌词并不是像计算机把信息存储在硬盘上那样的（译者注：计算机可以随机访问磁盘。）。你是顺序记忆的。知道了前一个字母，你很容易知道下一个。这是一种条件记忆，只有你最近知道了前一个记忆，你才容易想起来下一个记忆，就想你熟悉的链表一样。</p>\\n<p>但是，并不是说你不唱歌的时候，歌就不在你脑子里了。而是说你如果想直接跳到中间那部分，你会发现很难直接找到其在脑中的呈现（也许是一堆神经元）。你想直接搜索到一首歌的中间部分，这是很难的， 因为你以前没有这样做过，所以没有索引可以指向歌曲的中间部分。 就好比你邻居家有很多小路， 你从前门进去顺着路走很容易找到后院，但是让你直接到后院去就不太容易。想了解更过关于大脑的知识，请看<a href=\\\"http://www.human-memory.net/processes_recall.html\\\">这里</a>。</p>\\n<p>跟链表很像，记忆这样存储很高效。我们可以发现这样存储在解决很多问题时候有优势。</p>\\n<p>如果你的数据是一个序列，那么记忆就很重要（意味着你必须记住某些东西）。看下面的视频：</p>\\n<div>\\n          <div\\n            class=\\\"gatsby-resp-iframe-wrapper\\\"\\n            style=\\\"padding-bottom: 75%; position: relative; height: 0; overflow: hidden;margin-bottom: 1.0725rem\\\"\\n          >\\n            <iframe src=\\\"https://www.youtube.com/embed/UL0ZOgN2SqY\\\" frameborder=\\\"0\\\" allowfullscreen style=\\\"\\n            position: absolute;\\n            top: 0;\\n            left: 0;\\n            width: 100%;\\n            height: 100%;\\n          \\\"></iframe>\\n          </div>\\n          </div>\\n<p>每一个数据点就是视频中的一帧。如果你想训练一个神经网络来预测下一帧小球的位置， 那么知道上一帧小球的位置就很重要。这样的序列数据就是我们需要构建循环神经网络的原因。那么， 神经网络怎么记住以前的信息呢？</p>\\n<p>神经网络有隐藏层。一般而言，隐藏层的状态由输入决定。所以，一般而言神经网络的信息流如下图：</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">input -&gt; hidden -&gt; output</code></pre>\\n      </div>\\n<p>这很简单直接。特定的输入决定特定的隐藏层，特定的隐藏层又决定了输出。这是一种封闭系统。记忆改变了这种状况。记忆意味着，隐藏状态是由当前时间点的输入和上一个时间点的隐藏状态决定的。</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">(input + prev_hidden) -&gt; hidden -&gt; output</code></pre>\\n      </div>\\n<p>为什么是隐藏层而不是输入层呢？我们也可以这样做呀：</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">(input + prev_input) -&gt; hidden -&gt; output</code></pre>\\n      </div>\\n<p>现在，仔细想想，如果有四个时间点，如果我们采用隐藏层循环是如下图：\\n<img src=\\\"http://oml1i2pi6.bkt.clouddn.com/hidden-recurrence.jpg\\\" alt=\\\"hidden layer recurrence\\\">\\n如果采用输入层循环会是：\\n<img src=\\\"http://oml1i2pi6.bkt.clouddn.com/input-recurrence.jpg\\\" alt=\\\"input layer recurrence\\\">\\n看到区别没，隐藏层记忆了之前所有的输入信息，而输入层循环则只能利用到上一个输入。举个例子，假设一首歌词里面有”…I love you…”和”…I love carrots…”，如果采用输入层循环，则没法根据”I love”来预测下一个词是什么？因为当前输入是love，前一个输入是I，这两种情况一致，所以没法区分。 而隐藏层循环则可以记住更久之前的输入信息，因而能更好地预测下一个词。理论上而言，隐藏层循环可以记住所有之前的输入，当然记忆会随着时间流逝逐渐忘却。有兴趣的可以看<a href=\\\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\\\">这篇blog</a>。</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">            停下来好好想想， 直到你感觉想明白了再继续。</code></pre>\\n      </div>\\n<h1 id=\\\"第二部分：rnn---神经网络记忆\\\"><a href=\\\"#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%9Arnn---%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%B0%E5%BF%86\\\" aria-hidden=\\\"true\\\" class=\\\"anchor\\\"><svg aria-hidden=\\\"true\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg></a>第二部分：RNN - 神经网络记忆</h1>\\n<p>现在我们已经有了一些直观认识， 接下来让我们更进一步分析。正如在<a href=\\\"http://iamtrask.github.io/2015/07/12/basic-python-network/\\\">反向传播这篇blog</a>里介绍的，神经网络的输入层是由输入数据集决定的。每一行输入数据用来产生隐藏层（通过正向传播）。每个隐藏层又用于产生输出层（假设只有一层隐藏层）。如我们之前所说，记忆意味着隐藏层是由输入数据和前一次的隐藏层组合而成。怎么做的呢？很像神经网络里面其他传播的做法一样， 通过矩阵！这个矩阵定义了当前隐藏层跟前一个隐藏层的关系。</p>\\n<p><img src=\\\"http://iamtrask.github.io/img/basic_recurrence_singleton.png\\\" alt=\\\"rnn\\\">\\n这幅图中很重要的一点是有三个权重矩阵。有两个我们很熟悉了。SYNAPSE<em>0用于把输入数据传播到隐藏层。SYNAPSE</em>1把隐藏层传播到输出数据。新矩阵（SYNAPSE<em>h，用于循环）把当前的隐藏层（layer</em>1）传播到下一个时间点的隐藏层（还是layer_1）。</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">            停下来好好想想， 直到你感觉想明白了再继续。</code></pre>\\n      </div>\\n<p><img src=\\\"http://iamtrask.github.io/img/recurrence_gif.gif\\\" alt=\\\"forward\\\">\\n上面的gif图展示了循环神经网络的神奇之处以及一些很重要的性质。它展示了四个时间点隐藏层的情况。第一个时间点，隐藏层仅由输入数据决定。第二个时间点，隐藏层是由输入数据和第一个时间点的隐藏层共同决定的。以此类推。你应该注意到了，第四个时间点的时候，网络已经“满了”。所以大概第五个时间点来的时候，就要选择哪些记忆保留，哪些记忆覆盖。现实如此。这就是记忆“容量”的概念。如你所想，更大的隐藏层，就能记住更长时间的东西。同样，这就需要神经网络学会<strong>忘记不相关的记忆</strong>然后<strong>记住重要的记忆</strong>。第三步有没看出什么重要信息？为什么<strong>绿色</strong>的要比其他颜色的多呢？</p>\\n<p>另外要注意的是隐藏层夹在输入层和输出层中间，所以输出已经不仅仅取决于输入了。输入仅仅改变记忆，而输出仅仅依赖于记忆。有趣的是，如果2，3，4时间节点没有输入数据的话，隐藏层同样会随着时间流逝而变化。</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">        停下来好好想想，确保你明白了刚讲的内容。</code></pre>\\n      </div>\\n<h1 id=\\\"第三部分：基于时间的反向传播\\\"><a href=\\\"#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%EF%BC%9A%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD\\\" aria-hidden=\\\"true\\\" class=\\\"anchor\\\"><svg aria-hidden=\\\"true\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg></a>第三部分：基于时间的反向传播</h1>\\n<p>那么循环神经网络是怎么学习的呢？看看下面的图。黑色表示预测结果，明黄色表示错误，褐黄色表示导数。\\n<img src=\\\"http://iamtrask.github.io/img/backprop_through_time.gif\\\" alt=\\\"bp\\\">\\n网络通过从1到4的全部前向传播（可以是任意长度的整个序列），然后再从4到1的反向传播导数来学习。你可以把它看成一个有点变形的普通神经网络，除了我们在不同的地方共享权值（synapses 0,1,and h）。除了这点， 它就是一个普通的神经网络。</p>\\n<h1 id=\\\"我们的玩具代码\\\"><a href=\\\"#%E6%88%91%E4%BB%AC%E7%9A%84%E7%8E%A9%E5%85%B7%E4%BB%A3%E7%A0%81\\\" aria-hidden=\\\"true\\\" class=\\\"anchor\\\"><svg aria-hidden=\\\"true\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg></a>我们的玩具代码</h1>\\n<p>来，我们用循环神经网络做个模型来实现<strong>二进制加法</strong>。看到下面的图没，你猜猜顶上的彩色的1表示什么意思呢？\\n<img src=\\\"http://iamtrask.github.io/img/binary_addition.GIF\\\" alt=\\\"toy code\\\">\\n方框里的彩色的1表示<strong>进位</strong>。我们就要用循环神经网络来记住这个进位。求和的时候需要记住<strong>进位</strong>（如果不懂，可以看<a href=\\\"https://www.youtube.com/watch?v=jB_sRh5yoZk\\\">这里</a>）。</p>\\n<p>二进制加法做法就是，从右往左，根据上面两行的bit来预测第三行的bit为1还是0。我们想要神经网络遍历整个二进制序列记住是否有进位，以便能计算出正确的结果。不要太纠结这个问题本身，神经网络也不在乎这个问题。它在乎的只是每个时刻它会收到两个输入（0或者1），然后它会传递给用于记忆是否有进位的隐藏层。神经网络会把所有这些信息（输入和隐藏层的记忆）考虑进去，来对每一位（每个时间点）做出正确的预测。</p>\\n<hr>\\n<p>下面原文里面是针对每行代码做的注释， 为了方便阅读， 我直接把注释写到了代码里面， 便于大家阅读。</p>\\n<p>译者注：RNN在自然语言处理里面大量使用，包括机器翻译，对话系统，机器做诗词等，本文只是简单介绍了一下原理。后续我会写一些应用方面的文章，敬请期待。</p>\",\"timeToRead\":6,\"frontmatter\":{\"title\":\"所有人都能学会用Python写出RNN-LSTM代码\",\"date\":\"2017-03-09\",\"category\":\"ML\",\"tags\":[\"rnn\",\"neural network\",\"dl\",\"ml\"],\"math\":null}}},\"pathContext\":{\"prev\":{\"url\":\"/linear-regression/\",\"title\":\"linear-regression\"},\"slug\":\"/iamtrask-anyone-can-code-lstm/\",\"next\":{\"url\":\"/word2vec-first-try-md/\",\"title\":\"用word2vec分析中文维基语料库\"}}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---iamtrask-anyone-can-code-lstm-b0eda0efd4b3ed20fafa.js","module.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"Magicly's Blog\",\"author\":\"Magicly\"}},\"markdownRemark\":{\"id\":\"/Users/spring/Developer/Gatsby/springjphoenix.github.io/src/pages/iamtrask-anyone-can-code-lstm.md absPath of file >>> MarkdownRemark\",\"html\":\"<p>本文翻译自<a href=\\\"https://twitter.com/iamtrask\\\">@iamtrask</a>的<a href=\\\"http://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/\\\">Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN)</a>。本文作者已通过<a href=\\\"https://twitter.com/magicly007/with_replies\\\">twitter联系作者，获得授权</a>。</p>\\n<h1 id=\\\"概要\\\"><a href=\\\"#%E6%A6%82%E8%A6%81\\\" aria-hidden=\\\"true\\\" class=\\\"anchor\\\"><svg aria-hidden=\\\"true\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg></a>概要</h1>\\n<p>我通过玩具代码一边学习一边调试能达到最好的学习效果。本文通过一个简单的python实现，教会你循环神经网络。</p>\\n<p>原文作者<a href=\\\"https://twitter.com/iamtrask\\\">@iamtrask</a>说他会在twitter上继续发布第二部分LSTM，敬请关注。</p>\\n<!-- more -->\\n<h1 id=\\\"废话少说，-给我看看代码\\\"><a href=\\\"#%E5%BA%9F%E8%AF%9D%E5%B0%91%E8%AF%B4%EF%BC%8C-%E7%BB%99%E6%88%91%E7%9C%8B%E7%9C%8B%E4%BB%A3%E7%A0%81\\\" aria-hidden=\\\"true\\\" class=\\\"anchor\\\"><svg aria-hidden=\\\"true\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg></a>废话少说， 给我看看代码</h1>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">import</span> copy<span class=\\\"token punctuation\\\">,</span> numpy <span class=\\\"token keyword\\\">as</span> np\\nnp<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">.</span>seed<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#固定随机数生成器的种子，便于得到固定的输出，【译者注：完全是为了方便调试用的]</span>\\n\\n<span class=\\\"token comment\\\"># compute sigmoid nonlinearity</span>\\n<span class=\\\"token keyword\\\">def</span> <span class=\\\"token function\\\">sigmoid</span><span class=\\\"token punctuation\\\">(</span>x<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span> <span class=\\\"token comment\\\">#激活函数</span>\\n    output <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">1</span><span class=\\\"token operator\\\">/</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">1</span><span class=\\\"token operator\\\">+</span>np<span class=\\\"token punctuation\\\">.</span>exp<span class=\\\"token punctuation\\\">(</span><span class=\\\"token operator\\\">-</span>x<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span>\\n    <span class=\\\"token keyword\\\">return</span> output\\n\\n<span class=\\\"token comment\\\"># convert output of sigmoid function to its derivative</span>\\n<span class=\\\"token keyword\\\">def</span> <span class=\\\"token function\\\">sigmoid_output_to_derivative</span><span class=\\\"token punctuation\\\">(</span>output<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span><span class=\\\"token comment\\\">#激活函数的导数</span>\\n    <span class=\\\"token keyword\\\">return</span> output<span class=\\\"token operator\\\">*</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">1</span><span class=\\\"token operator\\\">-</span>output<span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># training dataset generation</span>\\nint2binary <span class=\\\"token operator\\\">=</span> <span class=\\\"token punctuation\\\">{</span><span class=\\\"token punctuation\\\">}</span> <span class=\\\"token comment\\\">#整数到其二进制表示的映射</span>\\nbinary_dim <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">8</span> <span class=\\\"token comment\\\">#暂时制作256以内的加法， 可以调大</span>\\n\\n<span class=\\\"token comment\\\">## 以下5行代码计算0-256的二进制表示</span>\\nlargest_number <span class=\\\"token operator\\\">=</span> <span class=\\\"token builtin\\\">pow</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">2</span><span class=\\\"token punctuation\\\">,</span>binary_dim<span class=\\\"token punctuation\\\">)</span>\\nbinary <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>unpackbits<span class=\\\"token punctuation\\\">(</span>\\n    np<span class=\\\"token punctuation\\\">.</span>array<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token builtin\\\">range</span><span class=\\\"token punctuation\\\">(</span>largest_number<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">,</span>dtype<span class=\\\"token operator\\\">=</span>np<span class=\\\"token punctuation\\\">.</span>uint8<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">.</span>T<span class=\\\"token punctuation\\\">,</span>axis<span class=\\\"token operator\\\">=</span><span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">)</span>\\n<span class=\\\"token keyword\\\">for</span> i <span class=\\\"token keyword\\\">in</span> <span class=\\\"token builtin\\\">range</span><span class=\\\"token punctuation\\\">(</span>largest_number<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span>\\n    int2binary<span class=\\\"token punctuation\\\">[</span>i<span class=\\\"token punctuation\\\">]</span> <span class=\\\"token operator\\\">=</span> binary<span class=\\\"token punctuation\\\">[</span>i<span class=\\\"token punctuation\\\">]</span>\\n\\n<span class=\\\"token comment\\\"># input variables</span>\\nalpha <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">0.1</span> <span class=\\\"token comment\\\">#学习速率</span>\\ninput_dim <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">2</span> <span class=\\\"token comment\\\">#因为我们是做两个数相加，每次会喂给神经网络两个bit，所以输入的维度是2</span>\\nhidden_dim <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">16</span> <span class=\\\"token comment\\\">#隐藏层的神经元节点数，远比理论值要大（译者注：理论上而言，应该一个节点就可以记住有无进位了，但我试了发现4的时候都没法收敛），你可以自己调整这个数，看看调大了是容易更快地收敛还是更慢</span>\\noutput_dim <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">1</span> <span class=\\\"token comment\\\">#我们的输出是一个数，所以维度为1</span>\\n\\n\\n<span class=\\\"token comment\\\"># initialize neural network weights</span>\\nsynapse_0 <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">2</span><span class=\\\"token operator\\\">*</span>np<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">(</span>input_dim<span class=\\\"token punctuation\\\">,</span>hidden_dim<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">-</span> <span class=\\\"token number\\\">1</span> <span class=\\\"token comment\\\">#输入层到隐藏层的转化矩阵，维度为2*16， 2是输入维度，16是隐藏层维度</span>\\nsynapse_1 <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">2</span><span class=\\\"token operator\\\">*</span>np<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">(</span>hidden_dim<span class=\\\"token punctuation\\\">,</span>output_dim<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">-</span> <span class=\\\"token number\\\">1</span>\\nsynapse_h <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">2</span><span class=\\\"token operator\\\">*</span>np<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">(</span>hidden_dim<span class=\\\"token punctuation\\\">,</span>hidden_dim<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">-</span> <span class=\\\"token number\\\">1</span>\\n<span class=\\\"token comment\\\"># 译者注：np.random.random产生的是[0,1)的随机数，2 * [0, 1) - 1 => [-1, 1)，</span>\\n<span class=\\\"token comment\\\"># 是为了有正有负更快地收敛，这涉及到如何初始化参数的问题，通常来说都是靠“经验”或者说“启发式规则”，说得直白一点就是“蒙的”！机器学习里面，超参数的选择，大部分都是这种情况，哈哈。。。</span>\\n<span class=\\\"token comment\\\"># 我自己试了一下用【0, 2)之间的随机数，貌似不能收敛，用[0,1)就可以，呵呵。。。</span>\\n\\n<span class=\\\"token comment\\\"># 以下三个分别对应三个矩阵的变化</span>\\nsynapse_0_update <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>zeros_like<span class=\\\"token punctuation\\\">(</span>synapse_0<span class=\\\"token punctuation\\\">)</span>\\nsynapse_1_update <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>zeros_like<span class=\\\"token punctuation\\\">(</span>synapse_1<span class=\\\"token punctuation\\\">)</span>\\nsynapse_h_update <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>zeros_like<span class=\\\"token punctuation\\\">(</span>synapse_h<span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># training logic</span>\\n<span class=\\\"token comment\\\"># 学习10000个例子</span>\\n<span class=\\\"token keyword\\\">for</span> j <span class=\\\"token keyword\\\">in</span> <span class=\\\"token builtin\\\">range</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">100000</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span>\\n    \\n    <span class=\\\"token comment\\\"># 下面6行代码，随机产生两个0-128的数字，并查出他们的二进制表示。为了避免相加之和超过256，这里选择两个0-128的数字</span>\\n    <span class=\\\"token comment\\\"># generate a simple addition problem (a + b = c)</span>\\n    a_int <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">.</span>randint<span class=\\\"token punctuation\\\">(</span>largest_number<span class=\\\"token operator\\\">/</span><span class=\\\"token number\\\">2</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\"># int version</span>\\n    a <span class=\\\"token operator\\\">=</span> int2binary<span class=\\\"token punctuation\\\">[</span>a_int<span class=\\\"token punctuation\\\">]</span> <span class=\\\"token comment\\\"># binary encoding</span>\\n\\n    b_int <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>random<span class=\\\"token punctuation\\\">.</span>randint<span class=\\\"token punctuation\\\">(</span>largest_number<span class=\\\"token operator\\\">/</span><span class=\\\"token number\\\">2</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\"># int version</span>\\n    b <span class=\\\"token operator\\\">=</span> int2binary<span class=\\\"token punctuation\\\">[</span>b_int<span class=\\\"token punctuation\\\">]</span> <span class=\\\"token comment\\\"># binary encoding</span>\\n\\n    <span class=\\\"token comment\\\"># true answer</span>\\n    c_int <span class=\\\"token operator\\\">=</span> a_int <span class=\\\"token operator\\\">+</span> b_int\\n    c <span class=\\\"token operator\\\">=</span> int2binary<span class=\\\"token punctuation\\\">[</span>c_int<span class=\\\"token punctuation\\\">]</span>\\n    \\n    <span class=\\\"token comment\\\"># where we'll store our best guess (binary encoded)</span>\\n    <span class=\\\"token comment\\\"># 存储神经网络的预测值</span>\\n    d <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>zeros_like<span class=\\\"token punctuation\\\">(</span>c<span class=\\\"token punctuation\\\">)</span>\\n\\n    overallError <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">0</span> <span class=\\\"token comment\\\">#每次把总误差清零</span>\\n    \\n    layer_2_deltas <span class=\\\"token operator\\\">=</span> <span class=\\\"token builtin\\\">list</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#存储每个时间点输出层的误差</span>\\n    layer_1_values <span class=\\\"token operator\\\">=</span> <span class=\\\"token builtin\\\">list</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#存储每个时间点隐藏层的值</span>\\n    layer_1_values<span class=\\\"token punctuation\\\">.</span>append<span class=\\\"token punctuation\\\">(</span>np<span class=\\\"token punctuation\\\">.</span>zeros<span class=\\\"token punctuation\\\">(</span>hidden_dim<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#一开始没有隐藏层，所以里面都是0</span>\\n    \\n    <span class=\\\"token comment\\\"># moving along the positions in the binary encoding</span>\\n    <span class=\\\"token keyword\\\">for</span> position <span class=\\\"token keyword\\\">in</span> <span class=\\\"token builtin\\\">range</span><span class=\\\"token punctuation\\\">(</span>binary_dim<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span><span class=\\\"token comment\\\">#循环遍历每一个二进制位</span>\\n        \\n        <span class=\\\"token comment\\\"># generate input and output</span>\\n        X <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>array<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token punctuation\\\">[</span>a<span class=\\\"token punctuation\\\">[</span>binary_dim <span class=\\\"token operator\\\">-</span> position <span class=\\\"token operator\\\">-</span> <span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">,</span>b<span class=\\\"token punctuation\\\">[</span>binary_dim <span class=\\\"token operator\\\">-</span> position <span class=\\\"token operator\\\">-</span> <span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token comment\\\">#从右到左，每次去两个输入数字的一个bit位</span>\\n        y <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>array<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token punctuation\\\">[</span>c<span class=\\\"token punctuation\\\">[</span>binary_dim <span class=\\\"token operator\\\">-</span> position <span class=\\\"token operator\\\">-</span> <span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">.</span>T<span class=\\\"token comment\\\">#正确答案</span>\\n\\n        <span class=\\\"token comment\\\"># hidden layer (input ~+ prev_hidden)</span>\\n        layer_1 <span class=\\\"token operator\\\">=</span> sigmoid<span class=\\\"token punctuation\\\">(</span>np<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>X<span class=\\\"token punctuation\\\">,</span>synapse_0<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">+</span> np<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>layer_1_values<span class=\\\"token punctuation\\\">[</span><span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">,</span>synapse_h<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token comment\\\">#（输入层 + 之前的隐藏层） -> 新的隐藏层，这是体现循环神经网络的最核心的地方！！！</span>\\n\\n        <span class=\\\"token comment\\\"># output layer (new binary representation)</span>\\n        layer_2 <span class=\\\"token operator\\\">=</span> sigmoid<span class=\\\"token punctuation\\\">(</span>np<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>layer_1<span class=\\\"token punctuation\\\">,</span>synapse_1<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#隐藏层 * 隐藏层到输出层的转化矩阵synapse_1 -> 输出层</span>\\n\\n        <span class=\\\"token comment\\\"># did we miss?... if so, by how much?</span>\\n        layer_2_error <span class=\\\"token operator\\\">=</span> y <span class=\\\"token operator\\\">-</span> layer_2 <span class=\\\"token comment\\\">#预测误差是多少</span>\\n        layer_2_deltas<span class=\\\"token punctuation\\\">.</span>append<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">(</span>layer_2_error<span class=\\\"token punctuation\\\">)</span><span class=\\\"token operator\\\">*</span>sigmoid_output_to_derivative<span class=\\\"token punctuation\\\">(</span>layer_2<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#我们把每一个时间点的误差导数都记录下来</span>\\n        overallError <span class=\\\"token operator\\\">+=</span> np<span class=\\\"token punctuation\\\">.</span><span class=\\\"token builtin\\\">abs</span><span class=\\\"token punctuation\\\">(</span>layer_2_error<span class=\\\"token punctuation\\\">[</span><span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token comment\\\">#总误差</span>\\n    \\n        <span class=\\\"token comment\\\"># decode estimate so we can print it out</span>\\n        d<span class=\\\"token punctuation\\\">[</span>binary_dim <span class=\\\"token operator\\\">-</span> position <span class=\\\"token operator\\\">-</span> <span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span> <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span><span class=\\\"token builtin\\\">round</span><span class=\\\"token punctuation\\\">(</span>layer_2<span class=\\\"token punctuation\\\">[</span><span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#记录下每一个预测bit位</span>\\n        \\n        <span class=\\\"token comment\\\"># store hidden layer so we can use it in the next timestep</span>\\n        layer_1_values<span class=\\\"token punctuation\\\">.</span>append<span class=\\\"token punctuation\\\">(</span>copy<span class=\\\"token punctuation\\\">.</span>deepcopy<span class=\\\"token punctuation\\\">(</span>layer_1<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token comment\\\">#记录下隐藏层的值，在下一个时间点用</span>\\n    \\n    future_layer_1_delta <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>zeros<span class=\\\"token punctuation\\\">(</span>hidden_dim<span class=\\\"token punctuation\\\">)</span>\\n    \\n    <span class=\\\"token comment\\\">#前面代码我们完成了所有时间点的正向传播以及计算最后一层的误差，现在我们要做的是反向传播，从最后一个时间点到第一个时间点</span>\\n    <span class=\\\"token keyword\\\">for</span> position <span class=\\\"token keyword\\\">in</span> <span class=\\\"token builtin\\\">range</span><span class=\\\"token punctuation\\\">(</span>binary_dim<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span>\\n        \\n        X <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>array<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token punctuation\\\">[</span>a<span class=\\\"token punctuation\\\">[</span>position<span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">,</span>b<span class=\\\"token punctuation\\\">[</span>position<span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token comment\\\">#最后一次的两个输入</span>\\n        layer_1 <span class=\\\"token operator\\\">=</span> layer_1_values<span class=\\\"token punctuation\\\">[</span><span class=\\\"token operator\\\">-</span>position<span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span> <span class=\\\"token comment\\\">#当前时间点的隐藏层</span>\\n        prev_layer_1 <span class=\\\"token operator\\\">=</span> layer_1_values<span class=\\\"token punctuation\\\">[</span><span class=\\\"token operator\\\">-</span>position<span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">2</span><span class=\\\"token punctuation\\\">]</span> <span class=\\\"token comment\\\">#前一个时间点的隐藏层</span>\\n        \\n        <span class=\\\"token comment\\\"># error at output layer</span>\\n        layer_2_delta <span class=\\\"token operator\\\">=</span> layer_2_deltas<span class=\\\"token punctuation\\\">[</span><span class=\\\"token operator\\\">-</span>position<span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span> <span class=\\\"token comment\\\">#当前时间点输出层导数</span>\\n        <span class=\\\"token comment\\\"># error at hidden layer</span>\\n        <span class=\\\"token comment\\\"># 通过后一个时间点（因为是反向传播）的隐藏层误差和当前时间点的输出层误差，计算当前时间点的隐藏层误差</span>\\n        layer_1_delta <span class=\\\"token operator\\\">=</span> <span class=\\\"token punctuation\\\">(</span>future_layer_1_delta<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>synapse_h<span class=\\\"token punctuation\\\">.</span>T<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">+</span> layer_2_delta<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>synapse_1<span class=\\\"token punctuation\\\">.</span>T<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> sigmoid_output_to_derivative<span class=\\\"token punctuation\\\">(</span>layer_1<span class=\\\"token punctuation\\\">)</span>\\n\\n        <span class=\\\"token comment\\\"># let's update all our weights so we can try again</span>\\n        <span class=\\\"token comment\\\"># 我们已经完成了当前时间点的反向传播误差计算， 可以构建更新矩阵了。但是我们并不会现在就更新权重矩阵，因为我们还要用他们计算前一个时间点的更新矩阵呢。</span>\\n        <span class=\\\"token comment\\\"># 所以要等到我们完成了所有反向传播误差计算， 才会真正的去更新权重矩阵，我们暂时把更新矩阵存起来。</span>\\n        <span class=\\\"token comment\\\"># 可以看这里了解更多关于反向传播的知识http://iamtrask.github.io/2015/07/12/basic-python-network/</span>\\n        synapse_1_update <span class=\\\"token operator\\\">+=</span> np<span class=\\\"token punctuation\\\">.</span>atleast_2d<span class=\\\"token punctuation\\\">(</span>layer_1<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">.</span>T<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>layer_2_delta<span class=\\\"token punctuation\\\">)</span>\\n        synapse_h_update <span class=\\\"token operator\\\">+=</span> np<span class=\\\"token punctuation\\\">.</span>atleast_2d<span class=\\\"token punctuation\\\">(</span>prev_layer_1<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">.</span>T<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>layer_1_delta<span class=\\\"token punctuation\\\">)</span>\\n        synapse_0_update <span class=\\\"token operator\\\">+=</span> X<span class=\\\"token punctuation\\\">.</span>T<span class=\\\"token punctuation\\\">.</span>dot<span class=\\\"token punctuation\\\">(</span>layer_1_delta<span class=\\\"token punctuation\\\">)</span>\\n        \\n        future_layer_1_delta <span class=\\\"token operator\\\">=</span> layer_1_delta\\n    \\n\\n    <span class=\\\"token comment\\\"># 我们已经完成了所有的反向传播，可以更新几个转换矩阵了。并把更新矩阵变量清零</span>\\n    synapse_0 <span class=\\\"token operator\\\">+=</span> synapse_0_update <span class=\\\"token operator\\\">*</span> alpha\\n    synapse_1 <span class=\\\"token operator\\\">+=</span> synapse_1_update <span class=\\\"token operator\\\">*</span> alpha\\n    synapse_h <span class=\\\"token operator\\\">+=</span> synapse_h_update <span class=\\\"token operator\\\">*</span> alpha\\n\\n    synapse_0_update <span class=\\\"token operator\\\">*=</span> <span class=\\\"token number\\\">0</span>\\n    synapse_1_update <span class=\\\"token operator\\\">*=</span> <span class=\\\"token number\\\">0</span>\\n    synapse_h_update <span class=\\\"token operator\\\">*=</span> <span class=\\\"token number\\\">0</span>\\n    \\n    <span class=\\\"token comment\\\"># print out progress</span>\\n    <span class=\\\"token keyword\\\">if</span><span class=\\\"token punctuation\\\">(</span>j <span class=\\\"token operator\\\">%</span> <span class=\\\"token number\\\">1000</span> <span class=\\\"token operator\\\">==</span> <span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span>\\n        <span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">\\\"Error:\\\"</span> <span class=\\\"token operator\\\">+</span> <span class=\\\"token builtin\\\">str</span><span class=\\\"token punctuation\\\">(</span>overallError<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span>\\n        <span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">\\\"Pred:\\\"</span> <span class=\\\"token operator\\\">+</span> <span class=\\\"token builtin\\\">str</span><span class=\\\"token punctuation\\\">(</span>d<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span>\\n        <span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">\\\"True:\\\"</span> <span class=\\\"token operator\\\">+</span> <span class=\\\"token builtin\\\">str</span><span class=\\\"token punctuation\\\">(</span>c<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span>\\n        out <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">0</span>\\n        <span class=\\\"token keyword\\\">for</span> index<span class=\\\"token punctuation\\\">,</span>x <span class=\\\"token keyword\\\">in</span> <span class=\\\"token builtin\\\">enumerate</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token builtin\\\">reversed</span><span class=\\\"token punctuation\\\">(</span>d<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span>\\n            out <span class=\\\"token operator\\\">+=</span> x<span class=\\\"token operator\\\">*</span><span class=\\\"token builtin\\\">pow</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">2</span><span class=\\\"token punctuation\\\">,</span>index<span class=\\\"token punctuation\\\">)</span>\\n        <span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token builtin\\\">str</span><span class=\\\"token punctuation\\\">(</span>a_int<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">+</span> <span class=\\\"token string\\\">\\\" + \\\"</span> <span class=\\\"token operator\\\">+</span> <span class=\\\"token builtin\\\">str</span><span class=\\\"token punctuation\\\">(</span>b_int<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">+</span> <span class=\\\"token string\\\">\\\" = \\\"</span> <span class=\\\"token operator\\\">+</span> <span class=\\\"token builtin\\\">str</span><span class=\\\"token punctuation\\\">(</span>out<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span>\\n        <span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">\\\"------------\\\"</span><span class=\\\"token punctuation\\\">)</span></code></pre>\\n      </div>\\n<h1 id=\\\"运行时输出\\\"><a href=\\\"#%E8%BF%90%E8%A1%8C%E6%97%B6%E8%BE%93%E5%87%BA\\\" aria-hidden=\\\"true\\\" class=\\\"anchor\\\"><svg aria-hidden=\\\"true\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg></a>运行时输出</h1>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">Error:[ 3.45638663]\\nPred:[0 0 0 0 0 0 0 1]\\nTrue:[0 1 0 0 0 1 0 1]\\n9 + 60 = 1\\n------------\\nError:[ 3.63389116]\\nPred:[1 1 1 1 1 1 1 1]\\nTrue:[0 0 1 1 1 1 1 1]\\n28 + 35 = 255\\n------------\\nError:[ 3.91366595]\\nPred:[0 1 0 0 1 0 0 0]\\nTrue:[1 0 1 0 0 0 0 0]\\n116 + 44 = 72\\n------------\\nError:[ 3.72191702]\\nPred:[1 1 0 1 1 1 1 1]\\nTrue:[0 1 0 0 1 1 0 1]\\n4 + 73 = 223\\n------------\\nError:[ 3.5852713]\\nPred:[0 0 0 0 1 0 0 0]\\nTrue:[0 1 0 1 0 0 1 0]\\n71 + 11 = 8\\n------------\\nError:[ 2.53352328]\\nPred:[1 0 1 0 0 0 1 0]\\nTrue:[1 1 0 0 0 0 1 0]\\n81 + 113 = 162\\n------------\\nError:[ 0.57691441]\\nPred:[0 1 0 1 0 0 0 1]\\nTrue:[0 1 0 1 0 0 0 1]\\n81 + 0 = 81\\n------------\\nError:[ 1.42589952]\\nPred:[1 0 0 0 0 0 0 1]\\nTrue:[1 0 0 0 0 0 0 1]\\n4 + 125 = 129\\n------------\\nError:[ 0.47477457]\\nPred:[0 0 1 1 1 0 0 0]\\nTrue:[0 0 1 1 1 0 0 0]\\n39 + 17 = 56\\n------------\\nError:[ 0.21595037]\\nPred:[0 0 0 0 1 1 1 0]\\nTrue:[0 0 0 0 1 1 1 0]\\n11 + 3 = 14\\n------------</code></pre>\\n      </div>\\n<h1 id=\\\"第一部分：什么是神经元记忆\\\"><a href=\\\"#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E5%85%83%E8%AE%B0%E5%BF%86\\\" aria-hidden=\\\"true\\\" class=\\\"anchor\\\"><svg aria-hidden=\\\"true\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg></a>第一部分：什么是神经元记忆</h1>\\n<p>顺着背出字母表，你很容易做到吧？</p>\\n<p>倒着背呢， 有点难哦。</p>\\n<p>试着想一首你记得的歌词。为什么顺着回忆比倒着回忆难？你能直接跳到第二小节的中间么？额， 好像有点难。 这是为什么呢？</p>\\n<p>这其实很符合逻辑。 你记忆字母表或者歌词并不是像计算机把信息存储在硬盘上那样的（译者注：计算机可以随机访问磁盘。）。你是顺序记忆的。知道了前一个字母，你很容易知道下一个。这是一种条件记忆，只有你最近知道了前一个记忆，你才容易想起来下一个记忆，就想你熟悉的链表一样。</p>\\n<p>但是，并不是说你不唱歌的时候，歌就不在你脑子里了。而是说你如果想直接跳到中间那部分，你会发现很难直接找到其在脑中的呈现（也许是一堆神经元）。你想直接搜索到一首歌的中间部分，这是很难的， 因为你以前没有这样做过，所以没有索引可以指向歌曲的中间部分。 就好比你邻居家有很多小路， 你从前门进去顺着路走很容易找到后院，但是让你直接到后院去就不太容易。想了解更过关于大脑的知识，请看<a href=\\\"http://www.human-memory.net/processes_recall.html\\\">这里</a>。</p>\\n<p>跟链表很像，记忆这样存储很高效。我们可以发现这样存储在解决很多问题时候有优势。</p>\\n<p>如果你的数据是一个序列，那么记忆就很重要（意味着你必须记住某些东西）。看下面的视频：</p>\\n<div>\\n          <div\\n            class=\\\"gatsby-resp-iframe-wrapper\\\"\\n            style=\\\"padding-bottom: 75%; position: relative; height: 0; overflow: hidden;margin-bottom: 1.0725rem\\\"\\n          >\\n            <iframe src=\\\"https://www.youtube.com/embed/UL0ZOgN2SqY\\\" frameborder=\\\"0\\\" allowfullscreen style=\\\"\\n            position: absolute;\\n            top: 0;\\n            left: 0;\\n            width: 100%;\\n            height: 100%;\\n          \\\"></iframe>\\n          </div>\\n          </div>\\n<p>每一个数据点就是视频中的一帧。如果你想训练一个神经网络来预测下一帧小球的位置， 那么知道上一帧小球的位置就很重要。这样的序列数据就是我们需要构建循环神经网络的原因。那么， 神经网络怎么记住以前的信息呢？</p>\\n<p>神经网络有隐藏层。一般而言，隐藏层的状态由输入决定。所以，一般而言神经网络的信息流如下图：</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">input -&gt; hidden -&gt; output</code></pre>\\n      </div>\\n<p>这很简单直接。特定的输入决定特定的隐藏层，特定的隐藏层又决定了输出。这是一种封闭系统。记忆改变了这种状况。记忆意味着，隐藏状态是由当前时间点的输入和上一个时间点的隐藏状态决定的。</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">(input + prev_hidden) -&gt; hidden -&gt; output</code></pre>\\n      </div>\\n<p>为什么是隐藏层而不是输入层呢？我们也可以这样做呀：</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">(input + prev_input) -&gt; hidden -&gt; output</code></pre>\\n      </div>\\n<p>现在，仔细想想，如果有四个时间点，如果我们采用隐藏层循环是如下图：\\n<img src=\\\"http://oml1i2pi6.bkt.clouddn.com/hidden-recurrence.jpg\\\" alt=\\\"hidden layer recurrence\\\">\\n如果采用输入层循环会是：\\n<img src=\\\"http://oml1i2pi6.bkt.clouddn.com/input-recurrence.jpg\\\" alt=\\\"input layer recurrence\\\">\\n看到区别没，隐藏层记忆了之前所有的输入信息，而输入层循环则只能利用到上一个输入。举个例子，假设一首歌词里面有”…I love you…”和”…I love carrots…”，如果采用输入层循环，则没法根据”I love”来预测下一个词是什么？因为当前输入是love，前一个输入是I，这两种情况一致，所以没法区分。 而隐藏层循环则可以记住更久之前的输入信息，因而能更好地预测下一个词。理论上而言，隐藏层循环可以记住所有之前的输入，当然记忆会随着时间流逝逐渐忘却。有兴趣的可以看<a href=\\\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\\\">这篇blog</a>。</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">            停下来好好想想， 直到你感觉想明白了再继续。</code></pre>\\n      </div>\\n<h1 id=\\\"第二部分：rnn---神经网络记忆\\\"><a href=\\\"#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%9Arnn---%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%B0%E5%BF%86\\\" aria-hidden=\\\"true\\\" class=\\\"anchor\\\"><svg aria-hidden=\\\"true\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg></a>第二部分：RNN - 神经网络记忆</h1>\\n<p>现在我们已经有了一些直观认识， 接下来让我们更进一步分析。正如在<a href=\\\"http://iamtrask.github.io/2015/07/12/basic-python-network/\\\">反向传播这篇blog</a>里介绍的，神经网络的输入层是由输入数据集决定的。每一行输入数据用来产生隐藏层（通过正向传播）。每个隐藏层又用于产生输出层（假设只有一层隐藏层）。如我们之前所说，记忆意味着隐藏层是由输入数据和前一次的隐藏层组合而成。怎么做的呢？很像神经网络里面其他传播的做法一样， 通过矩阵！这个矩阵定义了当前隐藏层跟前一个隐藏层的关系。</p>\\n<p><img src=\\\"http://iamtrask.github.io/img/basic_recurrence_singleton.png\\\" alt=\\\"rnn\\\">\\n这幅图中很重要的一点是有三个权重矩阵。有两个我们很熟悉了。SYNAPSE<em>0用于把输入数据传播到隐藏层。SYNAPSE</em>1把隐藏层传播到输出数据。新矩阵（SYNAPSE<em>h，用于循环）把当前的隐藏层（layer</em>1）传播到下一个时间点的隐藏层（还是layer_1）。</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">            停下来好好想想， 直到你感觉想明白了再继续。</code></pre>\\n      </div>\\n<p><img src=\\\"http://iamtrask.github.io/img/recurrence_gif.gif\\\" alt=\\\"forward\\\">\\n上面的gif图展示了循环神经网络的神奇之处以及一些很重要的性质。它展示了四个时间点隐藏层的情况。第一个时间点，隐藏层仅由输入数据决定。第二个时间点，隐藏层是由输入数据和第一个时间点的隐藏层共同决定的。以此类推。你应该注意到了，第四个时间点的时候，网络已经“满了”。所以大概第五个时间点来的时候，就要选择哪些记忆保留，哪些记忆覆盖。现实如此。这就是记忆“容量”的概念。如你所想，更大的隐藏层，就能记住更长时间的东西。同样，这就需要神经网络学会<strong>忘记不相关的记忆</strong>然后<strong>记住重要的记忆</strong>。第三步有没看出什么重要信息？为什么<strong>绿色</strong>的要比其他颜色的多呢？</p>\\n<p>另外要注意的是隐藏层夹在输入层和输出层中间，所以输出已经不仅仅取决于输入了。输入仅仅改变记忆，而输出仅仅依赖于记忆。有趣的是，如果2，3，4时间节点没有输入数据的话，隐藏层同样会随着时间流逝而变化。</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">        停下来好好想想，确保你明白了刚讲的内容。</code></pre>\\n      </div>\\n<h1 id=\\\"第三部分：基于时间的反向传播\\\"><a href=\\\"#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%EF%BC%9A%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD\\\" aria-hidden=\\\"true\\\" class=\\\"anchor\\\"><svg aria-hidden=\\\"true\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg></a>第三部分：基于时间的反向传播</h1>\\n<p>那么循环神经网络是怎么学习的呢？看看下面的图。黑色表示预测结果，明黄色表示错误，褐黄色表示导数。\\n<img src=\\\"http://iamtrask.github.io/img/backprop_through_time.gif\\\" alt=\\\"bp\\\">\\n网络通过从1到4的全部前向传播（可以是任意长度的整个序列），然后再从4到1的反向传播导数来学习。你可以把它看成一个有点变形的普通神经网络，除了我们在不同的地方共享权值（synapses 0,1,and h）。除了这点， 它就是一个普通的神经网络。</p>\\n<h1 id=\\\"我们的玩具代码\\\"><a href=\\\"#%E6%88%91%E4%BB%AC%E7%9A%84%E7%8E%A9%E5%85%B7%E4%BB%A3%E7%A0%81\\\" aria-hidden=\\\"true\\\" class=\\\"anchor\\\"><svg aria-hidden=\\\"true\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg></a>我们的玩具代码</h1>\\n<p>来，我们用循环神经网络做个模型来实现<strong>二进制加法</strong>。看到下面的图没，你猜猜顶上的彩色的1表示什么意思呢？\\n<img src=\\\"http://iamtrask.github.io/img/binary_addition.GIF\\\" alt=\\\"toy code\\\">\\n方框里的彩色的1表示<strong>进位</strong>。我们就要用循环神经网络来记住这个进位。求和的时候需要记住<strong>进位</strong>（如果不懂，可以看<a href=\\\"https://www.youtube.com/watch?v=jB_sRh5yoZk\\\">这里</a>）。</p>\\n<p>二进制加法做法就是，从右往左，根据上面两行的bit来预测第三行的bit为1还是0。我们想要神经网络遍历整个二进制序列记住是否有进位，以便能计算出正确的结果。不要太纠结这个问题本身，神经网络也不在乎这个问题。它在乎的只是每个时刻它会收到两个输入（0或者1），然后它会传递给用于记忆是否有进位的隐藏层。神经网络会把所有这些信息（输入和隐藏层的记忆）考虑进去，来对每一位（每个时间点）做出正确的预测。</p>\\n<hr>\\n<p>下面原文里面是针对每行代码做的注释， 为了方便阅读， 我直接把注释写到了代码里面， 便于大家阅读。</p>\\n<p>译者注：RNN在自然语言处理里面大量使用，包括机器翻译，对话系统，机器做诗词等，本文只是简单介绍了一下原理。后续我会写一些应用方面的文章，敬请期待。</p>\",\"timeToRead\":6,\"frontmatter\":{\"title\":\"所有人都能学会用Python写出RNN-LSTM代码\",\"date\":\"2017-03-09\",\"category\":\"ML\",\"tags\":[\"rnn\",\"neural network\",\"dl\",\"ml\"],\"math\":null}}},\"pathContext\":{\"prev\":{\"url\":\"/linear-regression/\",\"title\":\"linear-regression\"},\"slug\":\"/iamtrask-anyone-can-code-lstm/\",\"next\":{\"url\":\"/word2vec-first-try-md/\",\"title\":\"用word2vec分析中文维基语料库\"}}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/iamtrask-anyone-can-code-lstm.json\n// module id = 615\n// module chunks = 190882071525408"],"sourceRoot":""}
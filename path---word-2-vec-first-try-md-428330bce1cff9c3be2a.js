webpackJsonp([0xa70d63fa768a],{732:function(n,a){n.exports={data:{site:{siteMetadata:{title:"Magicly's Blog",author:"Magicly"}},markdownRemark:{id:"/Users/spring/Developer/Gatsby/gatsby-blog/src/pages/word2vec-first-try-md.md absPath of file >>> MarkdownRemark",html:'<p>最近需要做一些自然语言处理的工作， 发现google推出的wrod2vec比较有意思，据说可以推算出king + man - woman = queue，感觉很nb啊， 后续可以拿来做文本分类、情绪分析、关键词提取等。本文记录一下在中文wiki语料库上做的实验。</p>\n<!-- more -->\n<h1 id="word2vec下载安装"><a href="#word2vec%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>word2vec下载安装</h1>\n<p>自从google推出了<a href="https://code.google.com/archive/p/word2vec/">word2vec</a>后，网上已经有众多实现，包括：</p>\n<ul>\n<li><a href="https://code.google.com/archive/p/word2vec/">google发布的c++版本</a></li>\n<li>python版<a href="https://radimrehurek.com/gensim/models/word2vec.html">Gensim</a></li>\n<li>java版<a href="https://deeplearning4j.org/word2vec">DeepLearning4J</a></li>\n<li><a href="https://github.com/danielfrg/word2vec">Python interface to Google word2vec</a>。</li>\n</ul>\n<p>我选用Gensim版本，python3.5.2。</p>\n<h1 id="数据集简介"><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E7%AE%80%E4%BB%8B" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>数据集简介</h1>\n<p>我们用的是中文wiki语料库，下载链接<a href="https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2%EF%BC%8C%E6%9C%891.3G%EF%BC%8C">https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2，有1.3G，</a> 解压之后是一个5.7G左右的xml文档。里面包含了标题、分类、正文部分等。</p>\n<h1 id="数据预处理"><a href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>数据预处理</h1>\n<p>上一步解压之后的xml文档我们没法直接用， 需要经过一系列的处理， 包括xml标签去除， 编码转换、简繁体转换、分词等。</p>\n<h2 id="xml内容提取"><a href="#xml%E5%86%85%E5%AE%B9%E6%8F%90%E5%8F%96" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>xml内容提取</h2>\n<p>你可以自己写正则表达式提取内容， 当然这个太费事了。 好在已经有人做了这个事情， Gensim里自带了提取wiki内容的工具，代码如下：</p>\n<div class="gatsby-highlight">\n      <pre class="language-python"><code class="language-python"><span class="token keyword">from</span> gensim<span class="token punctuation">.</span>corpora <span class="token keyword">import</span> WikiCorpus\n\nspace <span class="token operator">=</span> b<span class="token string">" "</span>\ni <span class="token operator">=</span> <span class="token number">0</span>\noutput <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">\'wiki-zh-article.txt\'</span><span class="token punctuation">,</span> <span class="token string">\'wb\'</span><span class="token punctuation">)</span>\nwiki <span class="token operator">=</span> WikiCorpus<span class="token punctuation">(</span><span class="token string">\'zhwiki-latest-pages-articles.xml.bz2\'</span><span class="token punctuation">,</span> lemmatize <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> dictionary <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">)</span>\n<span class="token keyword">for</span> text <span class="token keyword">in</span> wiki<span class="token punctuation">.</span>get_texts<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>\n    output<span class="token punctuation">.</span>write<span class="token punctuation">(</span>space<span class="token punctuation">.</span>join<span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token string">"\\n"</span><span class="token punctuation">)</span>\n    i <span class="token operator">=</span> i <span class="token operator">+</span> <span class="token number">1</span>\n    <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">%</span> <span class="token number">10000</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>\n        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Saved "</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">" articles"</span><span class="token punctuation">)</span>\n\noutput<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>\n<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Finished Saved "</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">" articles"</span><span class="token punctuation">)</span></code></pre>\n      </div>\n<p>代码在我的macpro上大概了运行了20多分钟， 得到280819行的文本， 每行为一篇文章。有个坑大家要小心， 输入文件是压缩文件bz2，直接传解压缩之后的xml文件是会报错的。</p>\n<p>另外， 我为了简洁，把不相关代码删除了，因此贴出来的代码，并不符合编程的最佳实践，请自行忽略这个问题。</p>\n<h2 id="简繁体换过"><a href="#%E7%AE%80%E7%B9%81%E4%BD%93%E6%8D%A2%E8%BF%87" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>简繁体换过</h2>\n<p>由于wiki语料库里简体、繁体都有，不统一的话对后面分词和跑模型准确率有影响， 所以先统一转化为简体。用到的工具是<a href="https://github.com/BYVoid/OpenCC">opencc</a>。</p>\n<div class="gatsby-highlight">\n      <pre class="language-text"><code class="language-text">opencc -i wiki-zh-article.txt -o wiki-zh-article-zhs.txt -c t2s.json</code></pre>\n      </div>\n<p>话说作者BYVoid也是超级大神， 据说小时候玩电脑，出现乱码， 于是自己写了opencc，然后提交给Linus， 被整合到linux kernel里了，那时候应该是小学还是初中吧。后来本科毕业去阿里面试，得到青睐，晚上流传了当时的面试记录。<img src="http://s12.sinaimg.cn/orignal/001OxbOzzy6EHEbsLWP8b" alt="http://s12.sinaimg.cn/orignal/001OxbOzzy6EHEbsLWP8b"></p>\n<h2 id="编码转化"><a href="#%E7%BC%96%E7%A0%81%E8%BD%AC%E5%8C%96" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>编码转化</h2>\n<p>网上说文件中包含非utf-8字符，需要用iconv处理一下。我忘了我之前在全量数据上跑得时候有没有遇到， 反正写文章的时候，我拿了1/10的数据跑没遇到问题。如果遇到了可以用<a href="https://zh.wikipedia.org/wiki/Iconv">iconv</a>一行命令解决：</p>\n<div class="gatsby-highlight">\n      <pre class="language-text"><code class="language-text">iconv -c -t utf-8 &lt; wiki-zh-article-zhs.txt &gt; wiki-zh-article-zhs-utf8.txt</code></pre>\n      </div>\n<h2 id="分词"><a href="#%E5%88%86%E8%AF%8D" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>分词</h2>\n<p>接下来就是做分词，比较好用的工具有<a href="https://github.com/fxsjy/jieba">结巴分词</a>、<a href="http://ictclas.nlpir.org/">中科院的ICTCLAS</a>、<a href="http://thulac.thunlp.org/">清华的THULAC</a>、<a href="https://github.com/FudanNLP/fnlp">复旦的FudanNLP</a>等。我选用了结巴，代码如下：</p>\n<div class="gatsby-highlight">\n      <pre class="language-python"><code class="language-python"><span class="token keyword">import</span> codecs\n<span class="token keyword">import</span> jieba\n\ninfile <span class="token operator">=</span> <span class="token string">\'wiki-zh-article-zhs.txt\'</span>\noutfile <span class="token operator">=</span> <span class="token string">\'wiki-zh-words.txt\'</span>\n\ndescsFile <span class="token operator">=</span> codecs<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>infile<span class="token punctuation">,</span> <span class="token string">\'rb\'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">\'utf-8\'</span><span class="token punctuation">)</span>\ni <span class="token operator">=</span> <span class="token number">0</span>\n<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>outfile<span class="token punctuation">,</span> <span class="token string">\'w\'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">\'utf-8\'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>\n    <span class="token keyword">for</span> line <span class="token keyword">in</span> descsFile<span class="token punctuation">:</span>\n        i <span class="token operator">+=</span> <span class="token number">1</span>\n        <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">10000</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>\n            <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span>\n        line <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>\n        words <span class="token operator">=</span> jieba<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>line<span class="token punctuation">)</span>\n        <span class="token keyword">for</span> word <span class="token keyword">in</span> words<span class="token punctuation">:</span>\n            f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>word <span class="token operator">+</span> <span class="token string">\' \'</span><span class="token punctuation">)</span>\n        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">\'\\n\'</span><span class="token punctuation">)</span></code></pre>\n      </div>\n<p>又跑几十分钟， 喝杯茶去。。。</p>\n<h1 id="gensim跑模型训练"><a href="#gensim%E8%B7%91%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Gensim跑模型训练</h1>\n<p>终于可以跑模型了，这次时间会更长，看你的电脑配置了，强烈建议约个妹纸出去看看电影逛逛街再回来等。据同事说，gensim有一个坑，在windows下不能用多核？！！！于是我在我的8核macpro下跑一个多小时的，他在windows下要跑七八个小时，哈哈哈哈。。。</p>\n<div class="gatsby-highlight">\n      <pre class="language-python"><code class="language-python"><span class="token keyword">import</span> multiprocessing\n\n<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models <span class="token keyword">import</span> Word2Vec\n<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>word2vec <span class="token keyword">import</span> LineSentence\n\ninp <span class="token operator">=</span> <span class="token string">\'wiki-zh-words.txt\'</span>\noutp1 <span class="token operator">=</span> <span class="token string">\'wiki-zh-model\'</span>\noutp2 <span class="token operator">=</span> <span class="token string">\'wiki-zh-vector\'</span>\n\nmodel <span class="token operator">=</span> Word2Vec<span class="token punctuation">(</span>LineSentence<span class="token punctuation">(</span>inp<span class="token punctuation">)</span><span class="token punctuation">,</span> size <span class="token operator">=</span> <span class="token number">400</span><span class="token punctuation">,</span> window <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">,</span> min_count <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">,</span> workers <span class="token operator">=</span> multiprocessing<span class="token punctuation">.</span>cpu_count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>\n\nmodel<span class="token punctuation">.</span>save<span class="token punctuation">(</span>outp1<span class="token punctuation">)</span> <span class="token comment">## 以二进制格式存储</span>\nmodel<span class="token punctuation">.</span>save_word2vec_format<span class="token punctuation">(</span>outp2<span class="token punctuation">,</span> binary <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token comment">## 以文本格式存储， 一行是一个词的vector</span></code></pre>\n      </div>\n<p>这里用save<em>word2vec</em>format纯粹是为了看看输出的模型是什么样子的， 这样：</p>\n<div class="gatsby-highlight">\n      <pre class="language-text"><code class="language-text">台湾 0.396402 1.611405 -0.291840 -0.951169 -0.109141 1.918246 0.215038 0.674539 2.335748 -0.757200 -0.290877 2.198100 -0.309420 0.438734 -1.731025 -0.233053 0.150694 2.214514 ......</code></pre>\n      </div>\n<p>即每个词一行， 后面是400个数字， 即将每一个词变为一个400维的向量。</p>\n<div class="gatsby-highlight">\n      <pre class="language-text"><code class="language-text">size is the dimensionality of the feature vectors.</code></pre>\n      </div>\n<h1 id="看看效果"><a href="#%E7%9C%8B%E7%9C%8B%E6%95%88%E6%9E%9C" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>看看效果</h1>\n<p>看看跟杜甫相关的词呢：</p>\n<div class="gatsby-highlight">\n      <pre class="language-python"><code class="language-python"><span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models <span class="token keyword">import</span> Word2Vec\n\nmodel <span class="token operator">=</span> Word2Vec<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">\'./wiki-zh-model\'</span><span class="token punctuation">)</span>\n<span class="token comment"># model = Word2Vec.load_word2vec_format(\'./wiki-zh-vector\', binary = False) # 如果之前用文本保存话， 用这个方法加载</span>\nres <span class="token operator">=</span> model<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token string">\'杜甫\'</span><span class="token punctuation">)</span>\n<span class="token keyword">print</span><span class="token punctuation">(</span>res<span class="token punctuation">)</span></code></pre>\n      </div>\n<div class="gatsby-highlight">\n      <pre class="language-text"><code class="language-text">[(&#39;白居易&#39;, 0.8842014074325562), (&#39;苏轼&#39;, 0.8444569706916809), (&#39;陆游&#39;, 0.8307716846466064), (&#39;一诗&#39;, 0.8290032148361206), (&#39;韩愈&#39;, 0.8263246417045593), (&#39;王勃&#39;, 0.8244832754135132), (&#39;陶渊明&#39;, 0.8243700861930847), (&#39;赋诗&#39;, 0.8211008906364441), (&#39;吟咏&#39;, 0.82026606798172), (&#39;辛弃疾&#39;, 0.8185226917266846)]</code></pre>\n      </div>\n<div class="gatsby-highlight">\n      <pre class="language-text"><code class="language-text">&gt;&gt;&gt; model.most_similar(&#39;语言学&#39;)\n[(&#39;语言学家&#39;, 0.7147563695907593), (&#39;民族学&#39;, 0.6887255907058716), (&#39;历史学&#39;, 0.6869072914123535), (&#39;比较语言学&#39;, 0.6818138360977173), (&#39;语音学&#39;, 0.6741021871566772), (&#39;音韵学&#39;, 0.6673719882965088), (&#39;语言所&#39;, 0.6434118747711182), (&#39;比较文学&#39;, 0.633540153503418), (&#39;人类学&#39;, 0.633027195930481), (&#39;方言学&#39;, 0.6314626336097717)]\n&gt;&gt;&gt; model.most_similar(&#39;林丹&#39;)\n[(&#39;谌龙&#39;, 0.9071081280708313), (&#39;鲍春来&#39;, 0.9035789966583252), (&#39;傅海峰&#39;, 0.8911731243133545), (&#39;蔡赟&#39;, 0.8886306285858154), (&#39;汪鑫&#39;, 0.8803133964538574), (&#39;李宗伟&#39;, 0.8767721652984619), (&#39;谢杏芳&#39;, 0.8706355690956116), (&#39;周蜜&#39;, 0.865954577922821), (&#39;李雪芮&#39;, 0.8658450841903687), (&#39;赵芸蕾&#39;, 0.8650676012039185)]\n&gt;&gt;&gt; model.most_similar(&#39;习近平&#39;)\n[(&#39;胡锦涛&#39;, 0.8577725291252136), (&#39;江泽民&#39;, 0.8138135075569153), (&#39;赵紫阳&#39;, 0.7295876741409302), (&#39;温家宝&#39;, 0.7284029722213745), (&#39;朱镕基&#39;, 0.7241271734237671), (&#39;邓小平&#39;, 0.7226930856704712), (&#39;李克强&#39;, 0.7181681990623474), (&#39;曾庆红&#39;, 0.6949223279953003), (&#39;周永康&#39;, 0.6847086548805237), (&#39;反腐&#39;, 0.681549072265625)]\n&gt;&gt;&gt; model.most_similar(positive=[&#39;中国&#39;, &#39;东京&#39;], negative=[&#39;日本&#39;])\n[(&#39;北京&#39;, 0.35159438848495483), (&#39;中央电视台&#39;, 0.3406861424446106), (&#39;辽艺版&#39;, 0.3394508361816406), (&#39;宗藤&#39;, 0.32839435338974), (&#39;寻奇&#39;, 0.3166041970252991), (&#39;china&#39;, 0.3111165761947632), (&#39;是冈瓦&#39;, 0.3110591471195221), (&#39;北京电视台&#39;, 0.31081947684288025), (&#39;女热&#39;, 0.30060601234436035), (&#39;北京市&#39;, 0.29704713821411133)]\n&gt;&gt;&gt; model.most_similar(&#39;林志玲&#39;)\n[(&#39;伊能静&#39;, 0.7900516986846924), (&#39;柯震东&#39;, 0.787365198135376), (&#39;言承旭&#39;, 0.7779808044433594), (&#39;徐熙媛&#39;, 0.7775079607963562), (&#39;林志颖&#39;, 0.7681171894073486), (&#39;谢依霖&#39;, 0.7657250761985779), (&#39;阮经天&#39;, 0.7654315233230591), (&#39;郭书瑶&#39;, 0.7628788948059082), (&#39;张钧宁&#39;, 0.7612718939781189), (&#39;何润东&#39;, 0.7598745822906494)]</code></pre>\n      </div>\n<p>还有没有觉得这个可以拿来干点有意思的事情呢？比如：</p>\n<div class="gatsby-highlight">\n      <pre class="language-text"><code class="language-text">&gt;&gt;&gt; model.most_similar(&#39;苍井空&#39;)\n[(&#39;吉泽明步&#39;, 0.7175988554954529), (&#39;反町隆史&#39;, 0.6755084991455078), (&#39;金城武&#39;, 0.6724058389663696), (&#39;柴崎幸&#39;, 0.6579034924507141), (&#39;藤原纪香&#39;, 0.656890332698822), (&#39;松隆子&#39;, 0.6524500846862793), (&#39;仓田保昭&#39;, 0.6456934809684753), (&#39;柴咲幸&#39;, 0.6456423997879028), (&#39;叶山豪&#39;, 0.6449219584465027), (&#39;濑户朝香&#39;, 0.6442539095878601)]</code></pre>\n      </div>\n<p>请尽情发挥想象。。。。。。。。。。。。。。。。</p>\n<h1 id="其他"><a href="#%E5%85%B6%E4%BB%96" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>其他</h1>\n<p>列几个word2vec的用途。</p>\n<ul>\n<li>情感分析 <a href="http://datartisan.com/article/detail/48.html">http://datartisan.com/article/detail/48.html</a></li>\n<li>SEO <a href="https://seofangfa.com/seo-articles/word2vec.html">https://seofangfa.com/seo-articles/word2vec.html</a></li>\n</ul>\n<p>另外，如果想深入了解word2vec的原理， 可以看最初的论文<a href="https://arxiv.org/pdf/1310.4546.pdf">Distributed Representations of Words and Phrases\nand their Compositionality</a>，以及<a href="http://techblog.youdao.com/?p=915">有道的这篇分析</a>。</p>\n<p>如果想自己动手实现word2vec的话， 可以考虑用当前最热的深度学习框架<a href="https://www.tensorflow.org/">TensorFlow</a>来实现， 官网上有<a href="https://www.tensorflow.org/tutorials/word2vec">详细地介绍</a>。</p>\n<h1 id="refers"><a href="#refers" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Refers</h1>\n<ul>\n<li><a href="http://www.52nlp.cn/%E4%B8%AD%E8%8B%B1%E6%96%87%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%E8%AF%AD%E6%96%99%E4%B8%8A%E7%9A%84word2vec%E5%AE%9E%E9%AA%8C">http://www.52nlp.cn/%E4%B8%AD%E8%8B%B1%E6%96%87%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%E8%AF%AD%E6%96%99%E4%B8%8A%E7%9A%84word2vec%E5%AE%9E%E9%AA%8C</a></li>\n<li><a href="https://radimrehurek.com/gensim/models/word2vec.html">https://radimrehurek.com/gensim/models/word2vec.html</a></li>\n<li><a href="https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</a></li>\n<li><a href="http://licstar.net/archives/262">http://licstar.net/archives/262</a></li>\n<li><a href="https://github.com/fxsjy/jieba">https://github.com/fxsjy/jieba</a></li>\n<li><a href="http://thulac.thunlp.org/">http://thulac.thunlp.org/</a></li>\n<li><a href="http://arxiv.org/pdf/1310.4546.pdf">http://arxiv.org/pdf/1310.4546.pdf</a></li>\n<li><a href="https://www.tensorflow.org/tutorials/word2vec">https://www.tensorflow.org/tutorials/word2vec</a></li>\n<li><a href="http://techblog.youdao.com/?p=915">http://techblog.youdao.com/?p=915</a></li>\n<li><a href="http://blog.csdn.net/zhaoxinfan/article/details/11069485">http://blog.csdn.net/zhaoxinfan/article/details/11069485</a></li>\n<li><a href="http://cikuapi.com/index.php">http://cikuapi.com/index.php</a></li>\n</ul>',timeToRead:6,frontmatter:{title:"用word2vec分析中文维基语料库",date:"2017-03-03",category:"ML",tags:["machine learning","nlp","word2vec"],math:null}}},pathContext:{prev:{url:"/iamtrask-anyone-can-code-lstm/",title:"所有人都能学会用Python写出RNN-LSTM代码"},slug:"/word2vec-first-try-md/",next:{url:"/markdown-math/",title:"在markdown里如何写数学公式"}}}}});
//# sourceMappingURL=path---word-2-vec-first-try-md-428330bce1cff9c3be2a.js.map